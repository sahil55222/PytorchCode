{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7650840,"sourceType":"datasetVersion","datasetId":4460164},{"sourceId":7663592,"sourceType":"datasetVersion","datasetId":4468939}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":7857.641438,"end_time":"2024-02-21T12:07:26.257963","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-02-21T09:56:28.616525","version":"2.5.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"0a091777cbcf46d48ec4b40985034e4a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"36d4920274af415d8bf53e11c1a360a8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"697edc06b55f4366a956725353dee5d3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9e4bc58595e244dcb2632017f7b2a774","max":178675806,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b7e9f5f2eafc41098eb413d6de405d39","value":178675806}},"83a2894dc0ba4bc1866133cb24cbd68d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_36d4920274af415d8bf53e11c1a360a8","placeholder":"​","style":"IPY_MODEL_d1d45c41217243eebfeacc8948130f1f","value":"model.safetensors: 100%"}},"9e4bc58595e244dcb2632017f7b2a774":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b071d84e196142a0a4baefd53e2b2f4b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b7e9f5f2eafc41098eb413d6de405d39":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cb7b870fe658478cab0f6b7d8bb98cdd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b071d84e196142a0a4baefd53e2b2f4b","placeholder":"​","style":"IPY_MODEL_0a091777cbcf46d48ec4b40985034e4a","value":" 179M/179M [00:00&lt;00:00, 263MB/s]"}},"d1d45c41217243eebfeacc8948130f1f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ec6a4dc76066463490d1e30f06dbc6e4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_83a2894dc0ba4bc1866133cb24cbd68d","IPY_MODEL_697edc06b55f4366a956725353dee5d3","IPY_MODEL_cb7b870fe658478cab0f6b7d8bb98cdd"],"layout":"IPY_MODEL_f8c9b22bd8ff491ba9c7a849951dce06"}},"f8c9b22bd8ff491ba9c7a849951dce06":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}},"version_major":2,"version_minor":0}}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nfrom torch import nn\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader, SubsetRandomSampler\nfrom torchvision import transforms as T\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nimport numpy as np\n\n\n# now, for the model\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice\n# Configuration options\nk_folds = 5\nnum_epochs = 30\nloss_function = nn.CrossEntropyLoss()\n\n# For fold results\nresults = {'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n\n# Set fixed random number seed\ntorch.manual_seed(42)\n\n# Replace MNIST dataset with a custom image dataset\nimage_folder_path = \"/kaggle/input/augmented-20x-10x/20x/20x/augmented\"  # Change this to the path of your dataset\n\n# Additional transformation\ntransform = T.Compose([\n    T.RandomHorizontalFlip(),\n    T.RandomVerticalFlip(),\n    T.RandomApply(torch.nn.ModuleList([T.ColorJitter()]), p=0.20),\n    T.RandomRotation(degrees=(-45, 45)),\n    T.RandomAffine(degrees=0, translate=(0.2, 0.2), scale=(0.8, 1.2), shear=(-10, 10)),\n    T.Resize(256),\n    T.CenterCrop(224),\n    T.ToTensor(),\n    T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),  # imagenet means\n    # T.RandomErasing(p=0.2, value='random')\n])\n\ndataset = ImageFolder(root=image_folder_path, transform=transform)\n\n# Define the K-fold Cross Validator\nkfold = KFold(n_splits=k_folds, shuffle=True)\n\n# Load ResNet model\nimport timm\n\nmodel = timm.create_model('vgg16', pretrained=True)\n\nimport torch.nn as nn\n\n# Assuming 'model' is your existing model\n\n# Freeze all parameters in the existing model\nfor param in model.parameters():\n    param.requires_grad = False\n\n# Get the number of input features of the existing fc layer\nn_inputs = model.head.fc.in_features\n\n# Modify the fc layer and add more layers for reducing val loss\nmodel.head.fc = nn.Sequential(\n    nn.Linear(n_inputs, 1024),\n    nn.BatchNorm1d(1024),  # Batch Normalization\n    nn.ReLU(),\n    nn.Dropout(0.5),\n    nn.Linear(1024, 512),\n    nn.BatchNorm1d(512),   # Batch Normalization\n    nn.ReLU(),\n    nn.Dropout(0.5),\n    nn.Linear(512, 256),\n    nn.BatchNorm1d(256),   # Batch Normalization\n    nn.ReLU(),\n    nn.Dropout(0.5),\n    nn.Linear(256, 128),\n    nn.BatchNorm1d(128),   # Batch Normalization\n    nn.ReLU(),\n    nn.Dropout(0.5),\n    nn.Linear(128, 64),\n    nn.BatchNorm1d(64),    # Batch Normalization\n    nn.ReLU(),\n    nn.Dropout(0.5),\n    nn.Linear(64, 2)\n)\n\n# L2 regularization for all linear layers\nl2_lambda = 0.01\nfor layer in model.head.fc.children():\n    if isinstance(layer, nn.Linear):\n        layer.weight.data = nn.Parameter(layer.weight.data, requires_grad=True)\n        layer.bias.data = nn.Parameter(layer.bias.data, requires_grad=True)\n        layer.weight.data = layer.weight.data.add(-l2_lambda * layer.weight.data)\n\nmodel = model.to(device)\nprint('--------------------------------')\n\n# K-fold Cross Validation model evaluation\nfor fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n    # Print\n    print(f'FOLD {fold}')\n    print('--------------------------------')\n\n    # Sample elements randomly from a given list of ids, no replacement.\n    train_subsampler = SubsetRandomSampler(train_ids)\n    test_subsampler = SubsetRandomSampler(test_ids)\n\n    # Define data loaders for training and testing data in this fold\n    trainloader = DataLoader(dataset, batch_size=32, sampler=train_subsampler)\n    testloader = DataLoader(dataset, batch_size=32, sampler=test_subsampler)\n\n    # Initialize optimizer\n    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n\n    # Run the training loop for the defined number of epochs\n    for epoch in range(0, num_epochs):\n        # Print epoch\n        print(f'Starting epoch {epoch + 1}')\n\n        # Set current loss value\n        current_loss = 0.0\n\n        # Iterate over the DataLoader for training data\n        for i, data in enumerate(trainloader, 0):\n            # Get inputs\n            inputs, targets = data\n            inputs, targets = inputs.to('cuda'), targets.to('cuda')\n\n            # Zero the gradients\n            optimizer.zero_grad()\n\n            # Perform forward pass\n            outputs = model(inputs)\n\n            # Compute loss\n            loss = loss_function(outputs, targets)\n\n            # Perform backward pass\n            loss.backward()\n\n            # Perform optimization\n            optimizer.step()\n\n            # Print statistics\n            current_loss += loss.item()\n            if i % 500 == 499:\n                print('Loss after mini-batch %5d: %.3f' % (i + 1, current_loss / 500))\n                current_loss = 0.0\n\n    # Process is complete.\n    print('Training process has finished. Saving trained model.')\n\n    # Print about testing\n    print('Starting testing')\n\n    # Evaluation for this fold\n    correct, total = 0, 0\n    predictions = []\n    targets_list = []\n    with torch.no_grad():\n        # Iterate over the test data and generate predictions\n        for i, data in enumerate(testloader, 0):\n            # Get inputs\n            inputs, targets = data\n            inputs, targets = inputs.to('cuda'), targets.to('cuda')\n\n            # Generate outputs\n            outputs = model(inputs)\n\n            # Set total and correct\n            _, predicted = torch.max(outputs.data, 1)\n            total += targets.size(0)\n            correct += (predicted == targets).sum().item()\n\n            # Save predictions and targets for later calculation of precision, recall, f1\n            predictions.extend(predicted.cpu().numpy())\n            targets_list.extend(targets.cpu().numpy())\n\n        # Calculate precision, recall, f1-score\n        precision = precision_score(targets_list, predictions, average='macro')\n        recall = recall_score(targets_list, predictions, average='macro')\n        f1 = f1_score(targets_list, predictions, average='macro')\n\n        # Print metrics\n        print('Accuracy for fold %d: %d %%' % (fold, 100.0 * correct / total))\n        print('Precision for fold %d: %.3f' % (fold, precision))\n        print('Recall for fold %d: %.3f' % (fold, recall))\n        print('F1-score for fold %d: %.3f' % (fold, f1))\n        print('--------------------------------')\n\n        # Save metrics\n        results['accuracy'].append(100.0 * (correct / total))\n        results['precision'].append(precision)\n        results['recall'].append(recall)\n        results['f1'].append(f1)\n\n# Print fold results\nprint(f'K-FOLD CROSS VALIDATION RESULTS FOR {k_folds} FOLDS')\nprint('--------------------------------')\nfor metric, values in results.items():\n    avg_metric = np.mean(values)\n    std_metric = np.std(values)\n    print(f'Average {metric}: {avg_metric:.3f} ± {std_metric:.3f}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nfrom torch import nn\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader, SubsetRandomSampler\nfrom torchvision import transforms as T\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nimport numpy as np\n\n\n# now, for the model\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice\n# Configuration options\nk_folds = 5\nnum_epochs = 30\nloss_function = nn.CrossEntropyLoss()\n\n# For fold results\nresults = {'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n\n# Set fixed random number seed\ntorch.manual_seed(42)\n\n# Replace MNIST dataset with a custom image dataset\nimage_folder_path = \"/kaggle/input/augmented-20x-10x/20x/20x/augmented\"  # Change this to the path of your dataset\n\n# Additional transformation\ntransform = T.Compose([\n    T.RandomHorizontalFlip(),\n    T.RandomVerticalFlip(),\n    T.RandomApply(torch.nn.ModuleList([T.ColorJitter()]), p=0.20),\n    T.RandomRotation(degrees=(-45, 45)),\n    T.RandomAffine(degrees=0, translate=(0.2, 0.2), scale=(0.8, 1.2), shear=(-10, 10)),\n    T.Resize(256),\n    T.CenterCrop(224),\n    T.ToTensor(),\n    T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),  # imagenet means\n    # T.RandomErasing(p=0.2, value='random')\n])\n\ndataset = ImageFolder(root=image_folder_path, transform=transform)\n\n# Define the K-fold Cross Validator\nkfold = KFold(n_splits=k_folds, shuffle=True)\n\n# Load ResNet model\nimport timm\n\nmodel = timm.create_model('xception71', pretrained=True)\n\nimport torch.nn as nn\n\n# Assuming 'model' is your existing model\n\n# Freeze all parameters in the existing model\nfor param in model.parameters():\n    param.requires_grad = False\n\n# Get the number of input features of the existing fc layer\nn_inputs = model.head.fc.in_features\n\n# Modify the fc layer and add more layers for reducing val loss\nmodel.head.fc = nn.Sequential(\n    nn.Linear(n_inputs, 1024),\n    nn.BatchNorm1d(1024),  # Batch Normalization\n    nn.ReLU(),\n    nn.Dropout(0.5),\n    nn.Linear(1024, 512),\n    nn.BatchNorm1d(512),   # Batch Normalization\n    nn.ReLU(),\n    nn.Dropout(0.5),\n    nn.Linear(512, 256),\n    nn.BatchNorm1d(256),   # Batch Normalization\n    nn.ReLU(),\n    nn.Dropout(0.5),\n    nn.Linear(256, 128),\n    nn.BatchNorm1d(128),   # Batch Normalization\n    nn.ReLU(),\n    nn.Dropout(0.5),\n    nn.Linear(128, 64),\n    nn.BatchNorm1d(64),    # Batch Normalization\n    nn.ReLU(),\n    nn.Dropout(0.5),\n    nn.Linear(64, 2)\n)\n\n# L2 regularization for all linear layers\nl2_lambda = 0.01\nfor layer in model.head.fc.children():\n    if isinstance(layer, nn.Linear):\n        layer.weight.data = nn.Parameter(layer.weight.data, requires_grad=True)\n        layer.bias.data = nn.Parameter(layer.bias.data, requires_grad=True)\n        layer.weight.data = layer.weight.data.add(-l2_lambda * layer.weight.data)\n\nmodel = model.to(device)\nprint('--------------------------------')\n\n# K-fold Cross Validation model evaluation\nfor fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n    # Print\n    print(f'FOLD {fold}')\n    print('--------------------------------')\n\n    # Sample elements randomly from a given list of ids, no replacement.\n    train_subsampler = SubsetRandomSampler(train_ids)\n    test_subsampler = SubsetRandomSampler(test_ids)\n\n    # Define data loaders for training and testing data in this fold\n    trainloader = DataLoader(dataset, batch_size=32, sampler=train_subsampler)\n    testloader = DataLoader(dataset, batch_size=32, sampler=test_subsampler)\n\n    # Initialize optimizer\n    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n\n    # Run the training loop for the defined number of epochs\n    for epoch in range(0, num_epochs):\n        # Print epoch\n        print(f'Starting epoch {epoch + 1}')\n\n        # Set current loss value\n        current_loss = 0.0\n\n        # Iterate over the DataLoader for training data\n        for i, data in enumerate(trainloader, 0):\n            # Get inputs\n            inputs, targets = data\n            inputs, targets = inputs.to('cuda'), targets.to('cuda')\n\n            # Zero the gradients\n            optimizer.zero_grad()\n\n            # Perform forward pass\n            outputs = model(inputs)\n\n            # Compute loss\n            loss = loss_function(outputs, targets)\n\n            # Perform backward pass\n            loss.backward()\n\n            # Perform optimization\n            optimizer.step()\n\n            # Print statistics\n            current_loss += loss.item()\n            if i % 500 == 499:\n                print('Loss after mini-batch %5d: %.3f' % (i + 1, current_loss / 500))\n                current_loss = 0.0\n\n    # Process is complete.\n    print('Training process has finished. Saving trained model.')\n\n    # Print about testing\n    print('Starting testing')\n\n    # Evaluation for this fold\n    correct, total = 0, 0\n    predictions = []\n    targets_list = []\n    with torch.no_grad():\n        # Iterate over the test data and generate predictions\n        for i, data in enumerate(testloader, 0):\n            # Get inputs\n            inputs, targets = data\n            inputs, targets = inputs.to('cuda'), targets.to('cuda')\n\n            # Generate outputs\n            outputs = model(inputs)\n\n            # Set total and correct\n            _, predicted = torch.max(outputs.data, 1)\n            total += targets.size(0)\n            correct += (predicted == targets).sum().item()\n\n            # Save predictions and targets for later calculation of precision, recall, f1\n            predictions.extend(predicted.cpu().numpy())\n            targets_list.extend(targets.cpu().numpy())\n\n        # Calculate precision, recall, f1-score\n        precision = precision_score(targets_list, predictions, average='macro')\n        recall = recall_score(targets_list, predictions, average='macro')\n        f1 = f1_score(targets_list, predictions, average='macro')\n\n        # Print metrics\n        print('Accuracy for fold %d: %d %%' % (fold, 100.0 * correct / total))\n        print('Precision for fold %d: %.3f' % (fold, precision))\n        print('Recall for fold %d: %.3f' % (fold, recall))\n        print('F1-score for fold %d: %.3f' % (fold, f1))\n        print('--------------------------------')\n\n        # Save metrics\n        results['accuracy'].append(100.0 * (correct / total))\n        results['precision'].append(precision)\n        results['recall'].append(recall)\n        results['f1'].append(f1)\n\n# Print fold results\nprint(f'K-FOLD CROSS VALIDATION RESULTS FOR {k_folds} FOLDS')\nprint('--------------------------------')\nfor metric, values in results.items():\n    avg_metric = np.mean(values)\n    std_metric = np.std(values)\n    print(f'Average {metric}: {avg_metric:.3f} ± {std_metric:.3f}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nfrom torch import nn\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader, SubsetRandomSampler\nfrom torchvision import transforms as T\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nimport numpy as np\n\n\n# now, for the model\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice\n# Configuration options\nk_folds = 5\nnum_epochs = 30\nloss_function = nn.CrossEntropyLoss()\n\n# For fold results\nresults = {'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n\n# Set fixed random number seed\ntorch.manual_seed(42)\n\n# Replace MNIST dataset with a custom image dataset\nimage_folder_path = \"/kaggle/input/augmented-20x-10x/20x/20x/augmented\"  # Change this to the path of your dataset\n\n# Additional transformation\ntransform = T.Compose([\n    T.RandomHorizontalFlip(),\n    T.RandomVerticalFlip(),\n    T.RandomApply(torch.nn.ModuleList([T.ColorJitter()]), p=0.20),\n    T.RandomRotation(degrees=(-45, 45)),\n    T.RandomAffine(degrees=0, translate=(0.2, 0.2), scale=(0.8, 1.2), shear=(-10, 10)),\n    T.Resize(256),\n    T.CenterCrop(224),\n    T.ToTensor(),\n    T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),  # imagenet means\n    # T.RandomErasing(p=0.2, value='random')\n])\n\ndataset = ImageFolder(root=image_folder_path, transform=transform)\n\n# Define the K-fold Cross Validator\nkfold = KFold(n_splits=k_folds, shuffle=True)\n\n# Load ResNet model\nimport timm\n\nmodel = timm.create_model('inception_v3', pretrained=True)\n\nimport torch.nn as nn\n\n# Assuming 'model' is your existing model\n\n# Freeze all parameters in the existing model\nfor param in model.parameters():\n    param.requires_grad = False\n\n# Get the number of input features of the existing fc layer\nn_inputs = model.fc.in_features\n\n# Modify the fc layer and add more layers for reducing val loss\nmodel.fc = nn.Sequential(\n    nn.Linear(n_inputs, 1024),\n    nn.BatchNorm1d(1024),  # Batch Normalization\n    nn.ReLU(),\n    nn.Dropout(0.5),\n    nn.Linear(1024, 512),\n    nn.BatchNorm1d(512),   # Batch Normalization\n    nn.ReLU(),\n    nn.Dropout(0.5),\n    nn.Linear(512, 256),\n    nn.BatchNorm1d(256),   # Batch Normalization\n    nn.ReLU(),\n    nn.Dropout(0.5),\n    nn.Linear(256, 128),\n    nn.BatchNorm1d(128),   # Batch Normalization\n    nn.ReLU(),\n    nn.Dropout(0.5),\n    nn.Linear(128, 64),\n    nn.BatchNorm1d(64),    # Batch Normalization\n    nn.ReLU(),\n    nn.Dropout(0.5),\n    nn.Linear(64, 2)\n)\n\n# L2 regularization for all linear layers\nl2_lambda = 0.01\nfor layer in model.fc.children():\n    if isinstance(layer, nn.Linear):\n        layer.weight.data = nn.Parameter(layer.weight.data, requires_grad=True)\n        layer.bias.data = nn.Parameter(layer.bias.data, requires_grad=True)\n        layer.weight.data = layer.weight.data.add(-l2_lambda * layer.weight.data)\n\nmodel = model.to(device)\nprint('--------------------------------')\n\n# K-fold Cross Validation model evaluation\nfor fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n    # Print\n    print(f'FOLD {fold}')\n    print('--------------------------------')\n\n    # Sample elements randomly from a given list of ids, no replacement.\n    train_subsampler = SubsetRandomSampler(train_ids)\n    test_subsampler = SubsetRandomSampler(test_ids)\n\n    # Define data loaders for training and testing data in this fold\n    trainloader = DataLoader(dataset, batch_size=32, sampler=train_subsampler)\n    testloader = DataLoader(dataset, batch_size=32, sampler=test_subsampler)\n\n    # Initialize optimizer\n    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n\n    # Run the training loop for the defined number of epochs\n    for epoch in range(0, num_epochs):\n        # Print epoch\n        print(f'Starting epoch {epoch + 1}')\n\n        # Set current loss value\n        current_loss = 0.0\n\n        # Iterate over the DataLoader for training data\n        for i, data in enumerate(trainloader, 0):\n            # Get inputs\n            inputs, targets = data\n            inputs, targets = inputs.to('cuda'), targets.to('cuda')\n\n            # Zero the gradients\n            optimizer.zero_grad()\n\n            # Perform forward pass\n            outputs = model(inputs)\n\n            # Compute loss\n            loss = loss_function(outputs, targets)\n\n            # Perform backward pass\n            loss.backward()\n\n            # Perform optimization\n            optimizer.step()\n\n            # Print statistics\n            current_loss += loss.item()\n            if i % 500 == 499:\n                print('Loss after mini-batch %5d: %.3f' % (i + 1, current_loss / 500))\n                current_loss = 0.0\n\n    # Process is complete.\n    print('Training process has finished. Saving trained model.')\n\n    # Print about testing\n    print('Starting testing')\n\n    # Evaluation for this fold\n    correct, total = 0, 0\n    predictions = []\n    targets_list = []\n    with torch.no_grad():\n        # Iterate over the test data and generate predictions\n        for i, data in enumerate(testloader, 0):\n            # Get inputs\n            inputs, targets = data\n            inputs, targets = inputs.to('cuda'), targets.to('cuda')\n\n            # Generate outputs\n            outputs = model(inputs)\n\n            # Set total and correct\n            _, predicted = torch.max(outputs.data, 1)\n            total += targets.size(0)\n            correct += (predicted == targets).sum().item()\n\n            # Save predictions and targets for later calculation of precision, recall, f1\n            predictions.extend(predicted.cpu().numpy())\n            targets_list.extend(targets.cpu().numpy())\n\n        # Calculate precision, recall, f1-score\n        precision = precision_score(targets_list, predictions, average='macro')\n        recall = recall_score(targets_list, predictions, average='macro')\n        f1 = f1_score(targets_list, predictions, average='macro')\n\n        # Print metrics\n        print('Accuracy for fold %d: %d %%' % (fold, 100.0 * correct / total))\n        print('Precision for fold %d: %.3f' % (fold, precision))\n        print('Recall for fold %d: %.3f' % (fold, recall))\n        print('F1-score for fold %d: %.3f' % (fold, f1))\n        print('--------------------------------')\n\n        # Save metrics\n        results['accuracy'].append(100.0 * (correct / total))\n        results['precision'].append(precision)\n        results['recall'].append(recall)\n        results['f1'].append(f1)\n\n# Print fold results\nprint(f'K-FOLD CROSS VALIDATION RESULTS FOR {k_folds} FOLDS')\nprint('--------------------------------')\nfor metric, values in results.items():\n    avg_metric = np.mean(values)\n    std_metric = np.std(values)\n    print(f'Average {metric}: {avg_metric:.3f} ± {std_metric:.3f}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nfrom torch import nn\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader, SubsetRandomSampler\nfrom torchvision import transforms as T\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nimport numpy as np\n\n\n# now, for the model\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice\n# Configuration options\nk_folds = 5\nnum_epochs = 30\nloss_function = nn.CrossEntropyLoss()\n\n# For fold results\nresults = {'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n\n# Set fixed random number seed\ntorch.manual_seed(42)\n\n# Replace MNIST dataset with a custom image dataset\nimage_folder_path = \"/kaggle/input/augmented-20x-10x/20x/20x/augmented\"  # Change this to the path of your dataset\n\n# Additional transformation\ntransform = T.Compose([\n    T.RandomHorizontalFlip(),\n    T.RandomVerticalFlip(),\n    T.RandomApply(torch.nn.ModuleList([T.ColorJitter()]), p=0.20),\n    T.RandomRotation(degrees=(-45, 45)),\n    T.RandomAffine(degrees=0, translate=(0.2, 0.2), scale=(0.8, 1.2), shear=(-10, 10)),\n    T.Resize(256),\n    T.CenterCrop(224),\n    T.ToTensor(),\n    T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),  # imagenet means\n    # T.RandomErasing(p=0.2, value='random')\n])\n\ndataset = ImageFolder(root=image_folder_path, transform=transform)\n\n# Define the K-fold Cross Validator\nkfold = KFold(n_splits=k_folds, shuffle=True)\n\n# Load ResNet model\nimport timm\n\nmodel = timm.create_model('inception_v3', pretrained=True)\n\nimport torch.nn as nn\n\n# Assuming 'model' is your existing model\n\n# Freeze all parameters in the existing model\nfor param in model.parameters():\n    param.requires_grad = False\n\n# Get the number of input features of the existing fc layer\nn_inputs = model.head.fc.in_features\n\n# Modify the fc layer and add more layers for reducing val loss\nmodel.head = nn.Sequential(\n    nn.Linear(n_inputs, 1024),\n    nn.BatchNorm1d(1024),  # Batch Normalization\n    nn.ReLU(),\n    nn.Dropout(0.5),\n    nn.Linear(1024, 512),\n    nn.BatchNorm1d(512),   # Batch Normalization\n    nn.ReLU(),\n    nn.Dropout(0.5),\n    nn.Linear(512, 256),\n    nn.BatchNorm1d(256),   # Batch Normalization\n    nn.ReLU(),\n    nn.Dropout(0.5),\n    nn.Linear(256, 128),\n    nn.BatchNorm1d(128),   # Batch Normalization\n    nn.ReLU(),\n    nn.Dropout(0.5),\n    nn.Linear(128, 64),\n    nn.BatchNorm1d(64),    # Batch Normalization\n    nn.ReLU(),\n    nn.Dropout(0.5),\n    nn.Linear(64, 2)\n)\n\n# L2 regularization for all linear layers\nl2_lambda = 0.01\nfor layer in model.head.fc.children():\n    if isinstance(layer, nn.Linear):\n        layer.weight.data = nn.Parameter(layer.weight.data, requires_grad=True)\n        layer.bias.data = nn.Parameter(layer.bias.data, requires_grad=True)\n        layer.weight.data = layer.weight.data.add(-l2_lambda * layer.weight.data)\n\nmodel = model.to(device)\nprint('--------------------------------')\n\n# K-fold Cross Validation model evaluation\nfor fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n    # Print\n    print(f'FOLD {fold}')\n    print('--------------------------------')\n\n    # Sample elements randomly from a given list of ids, no replacement.\n    train_subsampler = SubsetRandomSampler(train_ids)\n    test_subsampler = SubsetRandomSampler(test_ids)\n\n    # Define data loaders for training and testing data in this fold\n    trainloader = DataLoader(dataset, batch_size=32, sampler=train_subsampler)\n    testloader = DataLoader(dataset, batch_size=32, sampler=test_subsampler)\n\n    # Initialize optimizer\n    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n\n    # Run the training loop for the defined number of epochs\n    for epoch in range(0, num_epochs):\n        # Print epoch\n        print(f'Starting epoch {epoch + 1}')\n\n        # Set current loss value\n        current_loss = 0.0\n\n        # Iterate over the DataLoader for training data\n        for i, data in enumerate(trainloader, 0):\n            # Get inputs\n            inputs, targets = data\n            inputs, targets = inputs.to('cuda'), targets.to('cuda')\n\n            # Zero the gradients\n            optimizer.zero_grad()\n\n            # Perform forward pass\n            outputs = model(inputs)\n\n            # Compute loss\n            loss = loss_function(outputs, targets)\n\n            # Perform backward pass\n            loss.backward()\n\n            # Perform optimization\n            optimizer.step()\n\n            # Print statistics\n            current_loss += loss.item()\n            if i % 500 == 499:\n                print('Loss after mini-batch %5d: %.3f' % (i + 1, current_loss / 500))\n                current_loss = 0.0\n\n    # Process is complete.\n    print('Training process has finished. Saving trained model.')\n\n    # Print about testing\n    print('Starting testing')\n\n    # Evaluation for this fold\n    correct, total = 0, 0\n    predictions = []\n    targets_list = []\n    with torch.no_grad():\n        # Iterate over the test data and generate predictions\n        for i, data in enumerate(testloader, 0):\n            # Get inputs\n            inputs, targets = data\n            inputs, targets = inputs.to('cuda'), targets.to('cuda')\n\n            # Generate outputs\n            outputs = model(inputs)\n\n            # Set total and correct\n            _, predicted = torch.max(outputs.data, 1)\n            total += targets.size(0)\n            correct += (predicted == targets).sum().item()\n\n            # Save predictions and targets for later calculation of precision, recall, f1\n            predictions.extend(predicted.cpu().numpy())\n            targets_list.extend(targets.cpu().numpy())\n\n        # Calculate precision, recall, f1-score\n        precision = precision_score(targets_list, predictions, average='macro')\n        recall = recall_score(targets_list, predictions, average='macro')\n        f1 = f1_score(targets_list, predictions, average='macro')\n\n        # Print metrics\n        print('Accuracy for fold %d: %d %%' % (fold, 100.0 * correct / total))\n        print('Precision for fold %d: %.3f' % (fold, precision))\n        print('Recall for fold %d: %.3f' % (fold, recall))\n        print('F1-score for fold %d: %.3f' % (fold, f1))\n        print('--------------------------------')\n\n        # Save metrics\n        results['accuracy'].append(100.0 * (correct / total))\n        results['precision'].append(precision)\n        results['recall'].append(recall)\n        results['f1'].append(f1)\n\n# Print fold results\nprint(f'K-FOLD CROSS VALIDATION RESULTS FOR {k_folds} FOLDS')\nprint('--------------------------------')\nfor metric, values in results.items():\n    avg_metric = np.mean(values)\n    std_metric = np.std(values)\n    print(f'Average {metric}: {avg_metric:.3f} ± {std_metric:.3f}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nfrom torch import nn\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader, SubsetRandomSampler\nfrom torchvision import transforms as T\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nimport numpy as np\n\n\n# now, for the model\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice\n# Configuration options\nk_folds = 5\nnum_epochs = 30\nloss_function = nn.CrossEntropyLoss()\n\n# For fold results\nresults = {'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n\n# Set fixed random number seed\ntorch.manual_seed(42)\n\n# Replace MNIST dataset with a custom image dataset\nimage_folder_path = \"/kaggle/input/augmented-20x-10x/20x/20x/augmented\"  # Change this to the path of your dataset\n\n# Additional transformation\ntransform = T.Compose([\n    T.RandomHorizontalFlip(),\n    T.RandomVerticalFlip(),\n    T.RandomApply(torch.nn.ModuleList([T.ColorJitter()]), p=0.20),\n    T.RandomRotation(degrees=(-45, 45)),\n    T.RandomAffine(degrees=0, translate=(0.2, 0.2), scale=(0.8, 1.2), shear=(-10, 10)),\n    T.Resize(256),\n    T.CenterCrop(224),\n    T.ToTensor(),\n    T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),  # imagenet means\n    # T.RandomErasing(p=0.2, value='random')\n])\n\ndataset = ImageFolder(root=image_folder_path, transform=transform)\n\n# Define the K-fold Cross Validator\nkfold = KFold(n_splits=k_folds, shuffle=True)\n\n# Load ResNet model\nimport timm\n\nmodel = timm.create_model('mobilenetv2_140', pretrained=True)\n\nimport torch.nn as nn\n\n# Assuming 'model' is your existing model\n\n# Freeze all parameters in the existing model\nfor param in model.parameters():\n    param.requires_grad = False\n\n# Get the number of input features of the existing fc layer\nn_inputs = model.head.fc.in_features\n\n# Modify the fc layer and add more layers for reducing val loss\nmodel.head = nn.Sequential(\n    nn.Linear(n_inputs, 1024),\n    nn.BatchNorm1d(1024),  # Batch Normalization\n    nn.ReLU(),\n    nn.Dropout(0.5),\n    nn.Linear(1024, 512),\n    nn.BatchNorm1d(512),   # Batch Normalization\n    nn.ReLU(),\n    nn.Dropout(0.5),\n    nn.Linear(512, 256),\n    nn.BatchNorm1d(256),   # Batch Normalization\n    nn.ReLU(),\n    nn.Dropout(0.5),\n    nn.Linear(256, 128),\n    nn.BatchNorm1d(128),   # Batch Normalization\n    nn.ReLU(),\n    nn.Dropout(0.5),\n    nn.Linear(128, 64),\n    nn.BatchNorm1d(64),    # Batch Normalization\n    nn.ReLU(),\n    nn.Dropout(0.5),\n    nn.Linear(64, 2)\n)\n\n# L2 regularization for all linear layers\nl2_lambda = 0.01\nfor layer in model.head.fc.children():\n    if isinstance(layer, nn.Linear):\n        layer.weight.data = nn.Parameter(layer.weight.data, requires_grad=True)\n        layer.bias.data = nn.Parameter(layer.bias.data, requires_grad=True)\n        layer.weight.data = layer.weight.data.add(-l2_lambda * layer.weight.data)\n\nmodel = model.to(device)\nprint('--------------------------------')\n\n# K-fold Cross Validation model evaluation\nfor fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n    # Print\n    print(f'FOLD {fold}')\n    print('--------------------------------')\n\n    # Sample elements randomly from a given list of ids, no replacement.\n    train_subsampler = SubsetRandomSampler(train_ids)\n    test_subsampler = SubsetRandomSampler(test_ids)\n\n    # Define data loaders for training and testing data in this fold\n    trainloader = DataLoader(dataset, batch_size=32, sampler=train_subsampler)\n    testloader = DataLoader(dataset, batch_size=32, sampler=test_subsampler)\n\n    # Initialize optimizer\n    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n\n    # Run the training loop for the defined number of epochs\n    for epoch in range(0, num_epochs):\n        # Print epoch\n        print(f'Starting epoch {epoch + 1}')\n\n        # Set current loss value\n        current_loss = 0.0\n\n        # Iterate over the DataLoader for training data\n        for i, data in enumerate(trainloader, 0):\n            # Get inputs\n            inputs, targets = data\n            inputs, targets = inputs.to('cuda'), targets.to('cuda')\n\n            # Zero the gradients\n            optimizer.zero_grad()\n\n            # Perform forward pass\n            outputs = model(inputs)\n\n            # Compute loss\n            loss = loss_function(outputs, targets)\n\n            # Perform backward pass\n            loss.backward()\n\n            # Perform optimization\n            optimizer.step()\n\n            # Print statistics\n            current_loss += loss.item()\n            if i % 500 == 499:\n                print('Loss after mini-batch %5d: %.3f' % (i + 1, current_loss / 500))\n                current_loss = 0.0\n\n    # Process is complete.\n    print('Training process has finished. Saving trained model.')\n\n    # Print about testing\n    print('Starting testing')\n\n    # Evaluation for this fold\n    correct, total = 0, 0\n    predictions = []\n    targets_list = []\n    with torch.no_grad():\n        # Iterate over the test data and generate predictions\n        for i, data in enumerate(testloader, 0):\n            # Get inputs\n            inputs, targets = data\n            inputs, targets = inputs.to('cuda'), targets.to('cuda')\n\n            # Generate outputs\n            outputs = model(inputs)\n\n            # Set total and correct\n            _, predicted = torch.max(outputs.data, 1)\n            total += targets.size(0)\n            correct += (predicted == targets).sum().item()\n\n            # Save predictions and targets for later calculation of precision, recall, f1\n            predictions.extend(predicted.cpu().numpy())\n            targets_list.extend(targets.cpu().numpy())\n\n        # Calculate precision, recall, f1-score\n        precision = precision_score(targets_list, predictions, average='macro')\n        recall = recall_score(targets_list, predictions, average='macro')\n        f1 = f1_score(targets_list, predictions, average='macro')\n\n        # Print metrics\n        print('Accuracy for fold %d: %d %%' % (fold, 100.0 * correct / total))\n        print('Precision for fold %d: %.3f' % (fold, precision))\n        print('Recall for fold %d: %.3f' % (fold, recall))\n        print('F1-score for fold %d: %.3f' % (fold, f1))\n        print('--------------------------------')\n\n        # Save metrics\n        results['accuracy'].append(100.0 * (correct / total))\n        results['precision'].append(precision)\n        results['recall'].append(recall)\n        results['f1'].append(f1)\n\n# Print fold results\nprint(f'K-FOLD CROSS VALIDATION RESULTS FOR {k_folds} FOLDS')\nprint('--------------------------------')\nfor metric, values in results.items():\n    avg_metric = np.mean(values)\n    std_metric = np.std(values)\n    print(f'Average {metric}: {avg_metric:.3f} ± {std_metric:.3f}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nfrom torch import nn\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader, SubsetRandomSampler\nfrom torchvision import transforms as T\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nimport numpy as np\n\n\n# now, for the model\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice\n# Configuration options\nk_folds = 5\nnum_epochs = 30\nloss_function = nn.CrossEntropyLoss()\n\n# For fold results\nresults = {'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n\n# Set fixed random number seed\ntorch.manual_seed(42)\n\n# Replace MNIST dataset with a custom image dataset\nimage_folder_path = \"/kaggle/input/augmented-20x-10x/20x/20x/augmented\"  # Change this to the path of your dataset\n\n# Additional transformation\ntransform = T.Compose([\n    T.RandomHorizontalFlip(),\n    T.RandomVerticalFlip(),\n    T.RandomApply(torch.nn.ModuleList([T.ColorJitter()]), p=0.20),\n    T.RandomRotation(degrees=(-45, 45)),\n    T.RandomAffine(degrees=0, translate=(0.2, 0.2), scale=(0.8, 1.2), shear=(-10, 10)),\n    T.Resize(256),\n    T.CenterCrop(224),\n    T.ToTensor(),\n    T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),  # imagenet means\n    # T.RandomErasing(p=0.2, value='random')\n])\n\ndataset = ImageFolder(root=image_folder_path, transform=transform)\n\n# Define the K-fold Cross Validator\nkfold = KFold(n_splits=k_folds, shuffle=True)\n\n# Load ResNet model\nimport timm\n\nmodel = timm.create_model('mobilenetv2_140', pretrained=True)\n\nimport torch.nn as nn\n\n# Assuming 'model' is your existing model\n\n# Freeze all parameters in the existing model\nfor param in model.parameters():\n    param.requires_grad = False\n\n# Get the number of input features of the existing fc layer\nn_inputs = model.classifier.in_features\n\n# Modify the fc layer and add more layers for reducing val loss\nmodel.classifier = nn.Sequential(\n    nn.Linear(n_inputs, 1024),\n    nn.BatchNorm1d(1024),  # Batch Normalization\n    nn.ReLU(),\n    nn.Dropout(0.5),\n    nn.Linear(1024, 512),\n    nn.BatchNorm1d(512),   # Batch Normalization\n    nn.ReLU(),\n    nn.Dropout(0.5),\n    nn.Linear(512, 256),\n    nn.BatchNorm1d(256),   # Batch Normalization\n    nn.ReLU(),\n    nn.Dropout(0.5),\n    nn.Linear(256, 128),\n    nn.BatchNorm1d(128),   # Batch Normalization\n    nn.ReLU(),\n    nn.Dropout(0.5),\n    nn.Linear(128, 64),\n    nn.BatchNorm1d(64),    # Batch Normalization\n    nn.ReLU(),\n    nn.Dropout(0.5),\n    nn.Linear(64, 2)\n)\n\n# L2 regularization for all linear layers\nl2_lambda = 0.01\nfor layer in model.classifier.children():\n    if isinstance(layer, nn.Linear):\n        layer.weight.data = nn.Parameter(layer.weight.data, requires_grad=True)\n        layer.bias.data = nn.Parameter(layer.bias.data, requires_grad=True)\n        layer.weight.data = layer.weight.data.add(-l2_lambda * layer.weight.data)\n\nmodel = model.to(device)\nprint('--------------------------------')\n\n# K-fold Cross Validation model evaluation\nfor fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n    # Print\n    print(f'FOLD {fold}')\n    print('--------------------------------')\n\n    # Sample elements randomly from a given list of ids, no replacement.\n    train_subsampler = SubsetRandomSampler(train_ids)\n    test_subsampler = SubsetRandomSampler(test_ids)\n\n    # Define data loaders for training and testing data in this fold\n    trainloader = DataLoader(dataset, batch_size=32, sampler=train_subsampler)\n    testloader = DataLoader(dataset, batch_size=32, sampler=test_subsampler)\n\n    # Initialize optimizer\n    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n\n    # Run the training loop for the defined number of epochs\n    for epoch in range(0, num_epochs):\n        # Print epoch\n        print(f'Starting epoch {epoch + 1}')\n\n        # Set current loss value\n        current_loss = 0.0\n\n        # Iterate over the DataLoader for training data\n        for i, data in enumerate(trainloader, 0):\n            # Get inputs\n            inputs, targets = data\n            inputs, targets = inputs.to('cuda'), targets.to('cuda')\n\n            # Zero the gradients\n            optimizer.zero_grad()\n\n            # Perform forward pass\n            outputs = model(inputs)\n\n            # Compute loss\n            loss = loss_function(outputs, targets)\n\n            # Perform backward pass\n            loss.backward()\n\n            # Perform optimization\n            optimizer.step()\n\n            # Print statistics\n            current_loss += loss.item()\n            if i % 500 == 499:\n                print('Loss after mini-batch %5d: %.3f' % (i + 1, current_loss / 500))\n                current_loss = 0.0\n\n    # Process is complete.\n    print('Training process has finished. Saving trained model.')\n\n    # Print about testing\n    print('Starting testing')\n\n    # Evaluation for this fold\n    correct, total = 0, 0\n    predictions = []\n    targets_list = []\n    with torch.no_grad():\n        # Iterate over the test data and generate predictions\n        for i, data in enumerate(testloader, 0):\n            # Get inputs\n            inputs, targets = data\n            inputs, targets = inputs.to('cuda'), targets.to('cuda')\n\n            # Generate outputs\n            outputs = model(inputs)\n\n            # Set total and correct\n            _, predicted = torch.max(outputs.data, 1)\n            total += targets.size(0)\n            correct += (predicted == targets).sum().item()\n\n            # Save predictions and targets for later calculation of precision, recall, f1\n            predictions.extend(predicted.cpu().numpy())\n            targets_list.extend(targets.cpu().numpy())\n\n        # Calculate precision, recall, f1-score\n        precision = precision_score(targets_list, predictions, average='macro')\n        recall = recall_score(targets_list, predictions, average='macro')\n        f1 = f1_score(targets_list, predictions, average='macro')\n\n        # Print metrics\n        print('Accuracy for fold %d: %d %%' % (fold, 100.0 * correct / total))\n        print('Precision for fold %d: %.3f' % (fold, precision))\n        print('Recall for fold %d: %.3f' % (fold, recall))\n        print('F1-score for fold %d: %.3f' % (fold, f1))\n        print('--------------------------------')\n\n        # Save metrics\n        results['accuracy'].append(100.0 * (correct / total))\n        results['precision'].append(precision)\n        results['recall'].append(recall)\n        results['f1'].append(f1)\n\n# Print fold results\nprint(f'K-FOLD CROSS VALIDATION RESULTS FOR {k_folds} FOLDS')\nprint('--------------------------------')\nfor metric, values in results.items():\n    avg_metric = np.mean(values)\n    std_metric = np.std(values)\n    print(f'Average {metric}: {avg_metric:.3f} ± {std_metric:.3f}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nfrom torch import nn\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader, SubsetRandomSampler\nfrom torchvision import transforms as T\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nimport numpy as np\n\n\n# now, for the model\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice\n# Configuration options\nk_folds = 5\nnum_epochs = 30\nloss_function = nn.CrossEntropyLoss()\n\n# For fold results\nresults = {'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n\n# Set fixed random number seed\ntorch.manual_seed(42)\n\n# Replace MNIST dataset with a custom image dataset\nimage_folder_path = \"/kaggle/input/augmented-20x-10x/20x/20x/augmented\"  # Change this to the path of your dataset\n\n# Additional transformation\ntransform = T.Compose([\n    T.RandomHorizontalFlip(),\n    T.RandomVerticalFlip(),\n    T.RandomApply(torch.nn.ModuleList([T.ColorJitter()]), p=0.20),\n    T.RandomRotation(degrees=(-45, 45)),\n    T.RandomAffine(degrees=0, translate=(0.2, 0.2), scale=(0.8, 1.2), shear=(-10, 10)),\n    T.Resize(256),\n    T.CenterCrop(224),\n    T.ToTensor(),\n    T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),  # imagenet means\n    # T.RandomErasing(p=0.2, value='random')\n])\n\ndataset = ImageFolder(root=image_folder_path, transform=transform)\n\n# Define the K-fold Cross Validator\nkfold = KFold(n_splits=k_folds, shuffle=True)\n\n# Load ResNet model\nimport timm\n\nmodel = timm.create_model('efficientnet_b4', pretrained=True)\n\nimport torch.nn as nn\n\n# Assuming 'model' is your existing model\n\n# Freeze all parameters in the existing model\nfor param in model.parameters():\n    param.requires_grad = False\n\n# Get the number of input features of the existing fc layer\nn_inputs = model.classifier.in_features\n\n# Modify the fc layer and add more layers for reducing val loss\nmodel.classifier = nn.Sequential(\n    nn.Linear(n_inputs, 1024),\n    nn.BatchNorm1d(1024),  # Batch Normalization\n    nn.ReLU(),\n    nn.Dropout(0.5),\n    nn.Linear(1024, 512),\n    nn.BatchNorm1d(512),   # Batch Normalization\n    nn.ReLU(),\n    nn.Dropout(0.5),\n    nn.Linear(512, 256),\n    nn.BatchNorm1d(256),   # Batch Normalization\n    nn.ReLU(),\n    nn.Dropout(0.5),\n    nn.Linear(256, 128),\n    nn.BatchNorm1d(128),   # Batch Normalization\n    nn.ReLU(),\n    nn.Dropout(0.5),\n    nn.Linear(128, 64),\n    nn.BatchNorm1d(64),    # Batch Normalization\n    nn.ReLU(),\n    nn.Dropout(0.5),\n    nn.Linear(64, 2)\n)\n\n# L2 regularization for all linear layers\nl2_lambda = 0.01\nfor layer in model.classifier.children():\n    if isinstance(layer, nn.Linear):\n        layer.weight.data = nn.Parameter(layer.weight.data, requires_grad=True)\n        layer.bias.data = nn.Parameter(layer.bias.data, requires_grad=True)\n        layer.weight.data = layer.weight.data.add(-l2_lambda * layer.weight.data)\n\nmodel = model.to(device)\nprint('--------------------------------')\n\n# K-fold Cross Validation model evaluation\nfor fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n    # Print\n    print(f'FOLD {fold}')\n    print('--------------------------------')\n\n    # Sample elements randomly from a given list of ids, no replacement.\n    train_subsampler = SubsetRandomSampler(train_ids)\n    test_subsampler = SubsetRandomSampler(test_ids)\n\n    # Define data loaders for training and testing data in this fold\n    trainloader = DataLoader(dataset, batch_size=32, sampler=train_subsampler)\n    testloader = DataLoader(dataset, batch_size=32, sampler=test_subsampler)\n\n    # Initialize optimizer\n    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n\n    # Run the training loop for the defined number of epochs\n    for epoch in range(0, num_epochs):\n        # Print epoch\n        print(f'Starting epoch {epoch + 1}')\n\n        # Set current loss value\n        current_loss = 0.0\n\n        # Iterate over the DataLoader for training data\n        for i, data in enumerate(trainloader, 0):\n            # Get inputs\n            inputs, targets = data\n            inputs, targets = inputs.to('cuda'), targets.to('cuda')\n\n            # Zero the gradients\n            optimizer.zero_grad()\n\n            # Perform forward pass\n            outputs = model(inputs)\n\n            # Compute loss\n            loss = loss_function(outputs, targets)\n\n            # Perform backward pass\n            loss.backward()\n\n            # Perform optimization\n            optimizer.step()\n\n            # Print statistics\n            current_loss += loss.item()\n            if i % 500 == 499:\n                print('Loss after mini-batch %5d: %.3f' % (i + 1, current_loss / 500))\n                current_loss = 0.0\n\n    # Process is complete.\n    print('Training process has finished. Saving trained model.')\n\n    # Print about testing\n    print('Starting testing')\n\n    # Evaluation for this fold\n    correct, total = 0, 0\n    predictions = []\n    targets_list = []\n    with torch.no_grad():\n        # Iterate over the test data and generate predictions\n        for i, data in enumerate(testloader, 0):\n            # Get inputs\n            inputs, targets = data\n            inputs, targets = inputs.to('cuda'), targets.to('cuda')\n\n            # Generate outputs\n            outputs = model(inputs)\n\n            # Set total and correct\n            _, predicted = torch.max(outputs.data, 1)\n            total += targets.size(0)\n            correct += (predicted == targets).sum().item()\n\n            # Save predictions and targets for later calculation of precision, recall, f1\n            predictions.extend(predicted.cpu().numpy())\n            targets_list.extend(targets.cpu().numpy())\n\n        # Calculate precision, recall, f1-score\n        precision = precision_score(targets_list, predictions, average='macro')\n        recall = recall_score(targets_list, predictions, average='macro')\n        f1 = f1_score(targets_list, predictions, average='macro')\n\n        # Print metrics\n        print('Accuracy for fold %d: %d %%' % (fold, 100.0 * correct / total))\n        print('Precision for fold %d: %.3f' % (fold, precision))\n        print('Recall for fold %d: %.3f' % (fold, recall))\n        print('F1-score for fold %d: %.3f' % (fold, f1))\n        print('--------------------------------')\n\n        # Save metrics\n        results['accuracy'].append(100.0 * (correct / total))\n        results['precision'].append(precision)\n        results['recall'].append(recall)\n        results['f1'].append(f1)\n\n# Print fold results\nprint(f'K-FOLD CROSS VALIDATION RESULTS FOR {k_folds} FOLDS')\nprint('--------------------------------')\nfor metric, values in results.items():\n    avg_metric = np.mean(values)\n    std_metric = np.std(values)\n    print(f'Average {metric}: {avg_metric:.3f} ± {std_metric:.3f}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nfrom torch import nn\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader, SubsetRandomSampler\nfrom torchvision import transforms as T\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nimport numpy as np\n\n\n# now, for the model\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice\n# Configuration options\nk_folds = 5\nnum_epochs = 30\nloss_function = nn.CrossEntropyLoss()\n\n# For fold results\nresults = {'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n\n# Set fixed random number seed\ntorch.manual_seed(42)\n\n# Replace MNIST dataset with a custom image dataset\nimage_folder_path = \"/kaggle/input/augmented-20x-10x/20x/20x/augmented\"  # Change this to the path of your dataset\n\n# Additional transformation\ntransform = T.Compose([\n    T.RandomHorizontalFlip(),\n    T.RandomVerticalFlip(),\n    T.RandomApply(torch.nn.ModuleList([T.ColorJitter()]), p=0.20),\n    T.RandomRotation(degrees=(-45, 45)),\n    T.RandomAffine(degrees=0, translate=(0.2, 0.2), scale=(0.8, 1.2), shear=(-10, 10)),\n    T.Resize(256),\n    T.CenterCrop(224),\n    T.ToTensor(),\n    T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),  # imagenet means\n    # T.RandomErasing(p=0.2, value='random')\n])\n\ndataset = ImageFolder(root=image_folder_path, transform=transform)\n\n# Define the K-fold Cross Validator\nkfold = KFold(n_splits=k_folds, shuffle=True)\n\n# Load ResNet model\n\nimport torch\nmodel = torch.hub.load('pytorch/vision:v0.10.0', 'googlenet', pretrained=True)\n\nimport torch.nn as nn\n\n# Assuming 'model' is your existing model\n\n# Freeze all parameters in the existing model\nfor param in model.parameters():\n    param.requires_grad = False\n\n# Get the number of input features of the existing fc layer\nn_inputs = model.fc.in_features\n\n# Modify the fc layer and add more layers for reducing val loss\nmodel.fc = nn.Sequential(\n    nn.Linear(n_inputs, 1024),\n    nn.BatchNorm1d(1024),  # Batch Normalization\n    nn.ReLU(),\n    nn.Dropout(0.5),\n    nn.Linear(1024, 512),\n    nn.BatchNorm1d(512),   # Batch Normalization\n    nn.ReLU(),\n    nn.Dropout(0.5),\n    nn.Linear(512, 256),\n    nn.BatchNorm1d(256),   # Batch Normalization\n    nn.ReLU(),\n    nn.Dropout(0.5),\n    nn.Linear(256, 128),\n    nn.BatchNorm1d(128),   # Batch Normalization\n    nn.ReLU(),\n    nn.Dropout(0.5),\n    nn.Linear(128, 64),\n    nn.BatchNorm1d(64),    # Batch Normalization\n    nn.ReLU(),\n    nn.Dropout(0.5),\n    nn.Linear(64, 2)\n)\n\n# L2 regularization for all linear layers\nl2_lambda = 0.01\nfor layer in model.fc.children():\n    if isinstance(layer, nn.Linear):\n        layer.weight.data = nn.Parameter(layer.weight.data, requires_grad=True)\n        layer.bias.data = nn.Parameter(layer.bias.data, requires_grad=True)\n        layer.weight.data = layer.weight.data.add(-l2_lambda * layer.weight.data)\n\nmodel = model.to(device)\nprint('--------------------------------')\n\n# K-fold Cross Validation model evaluation\nfor fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n    # Print\n    print(f'FOLD {fold}')\n    print('--------------------------------')\n\n    # Sample elements randomly from a given list of ids, no replacement.\n    train_subsampler = SubsetRandomSampler(train_ids)\n    test_subsampler = SubsetRandomSampler(test_ids)\n\n    # Define data loaders for training and testing data in this fold\n    trainloader = DataLoader(dataset, batch_size=32, sampler=train_subsampler)\n    testloader = DataLoader(dataset, batch_size=32, sampler=test_subsampler)\n\n    # Initialize optimizer\n    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n\n    # Run the training loop for the defined number of epochs\n    for epoch in range(0, num_epochs):\n        # Print epoch\n        print(f'Starting epoch {epoch + 1}')\n\n        # Set current loss value\n        current_loss = 0.0\n\n        # Iterate over the DataLoader for training data\n        for i, data in enumerate(trainloader, 0):\n            # Get inputs\n            inputs, targets = data\n            inputs, targets = inputs.to('cuda'), targets.to('cuda')\n\n            # Zero the gradients\n            optimizer.zero_grad()\n\n            # Perform forward pass\n            outputs = model(inputs)\n\n            # Compute loss\n            loss = loss_function(outputs, targets)\n\n            # Perform backward pass\n            loss.backward()\n\n            # Perform optimization\n            optimizer.step()\n\n            # Print statistics\n            current_loss += loss.item()\n            if i % 500 == 499:\n                print('Loss after mini-batch %5d: %.3f' % (i + 1, current_loss / 500))\n                current_loss = 0.0\n\n    # Process is complete.\n    print('Training process has finished. Saving trained model.')\n\n    # Print about testing\n    print('Starting testing')\n\n    # Evaluation for this fold\n    correct, total = 0, 0\n    predictions = []\n    targets_list = []\n    with torch.no_grad():\n        # Iterate over the test data and generate predictions\n        for i, data in enumerate(testloader, 0):\n            # Get inputs\n            inputs, targets = data\n            inputs, targets = inputs.to('cuda'), targets.to('cuda')\n\n            # Generate outputs\n            outputs = model(inputs)\n\n            # Set total and correct\n            _, predicted = torch.max(outputs.data, 1)\n            total += targets.size(0)\n            correct += (predicted == targets).sum().item()\n\n            # Save predictions and targets for later calculation of precision, recall, f1\n            predictions.extend(predicted.cpu().numpy())\n            targets_list.extend(targets.cpu().numpy())\n\n        # Calculate precision, recall, f1-score\n        precision = precision_score(targets_list, predictions, average='macro')\n        recall = recall_score(targets_list, predictions, average='macro')\n        f1 = f1_score(targets_list, predictions, average='macro')\n\n        # Print metrics\n        print('Accuracy for fold %d: %d %%' % (fold, 100.0 * correct / total))\n        print('Precision for fold %d: %.3f' % (fold, precision))\n        print('Recall for fold %d: %.3f' % (fold, recall))\n        print('F1-score for fold %d: %.3f' % (fold, f1))\n        print('--------------------------------')\n\n        # Save metrics\n        results['accuracy'].append(100.0 * (correct / total))\n        results['precision'].append(precision)\n        results['recall'].append(recall)\n        results['f1'].append(f1)\n\n# Print fold results\nprint(f'K-FOLD CROSS VALIDATION RESULTS FOR {k_folds} FOLDS')\nprint('--------------------------------')\nfor metric, values in results.items():\n    avg_metric = np.mean(values)\n    std_metric = np.std(values)\n    print(f'Average {metric}: {avg_metric:.3f} ± {std_metric:.3f}')","metadata":{},"execution_count":null,"outputs":[]}]}