{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as T\nfrom torch.utils.data import DataLoader, Dataset, random_split\nfrom torchvision import datasets\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.random_projection import GaussianRandomProjection\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\nimport timm\n\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\n\n# Step 1: Define your custom dataset\nclass CustomDataset(Dataset):\n    def __init__(self, data, targets, transform=None):\n        self.data = data\n        self.targets = targets\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        sample = self.data[idx]\n        target = self.targets[idx]\n\n        if self.transform:\n            sample = self.transform(sample)\n\n        return sample, target\n\nseed = 12\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\n\n# Step 2: Define transformations for your custom dataset\ntransform = T.Compose([\n        T.RandomHorizontalFlip(),\n        T.RandomVerticalFlip(),\n        T.RandomApply(torch.nn.ModuleList([T.ColorJitter()]), p=0.10),\n        T.RandomAffine(degrees=(-30, 30), translate=(0.2, 0.2), scale=(0.8, 1.2), shear=(-10, 10)),\n        T.Resize(256),\n        T.CenterCrop(224),\n        T.ToTensor(),\n        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\ntest_transform = T.Compose([\n        T.Resize(256),\n        T.CenterCrop(224),\n        T.ToTensor(),\n        T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n])\n\n\n# Step 3: Define function to get data loaders for train, validation, and test datasets\ndef get_data_loaders(data_dir, test_dir, batch_size):\n    train_data = datasets.ImageFolder(os.path.join(data_dir), transform=transform)\n    test_data = datasets.ImageFolder(os.path.join(test_dir), transform=test_transform)\n\n    train_size = int(0.8 * len(train_data))\n    val_size = len(train_data) - train_size\n    train_dataset, val_dataset = random_split(train_data, [train_size, val_size])\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=4)\n\n    return train_loader, val_loader, test_loader\n\nimport timm\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# Step 4: Load pre-trained PyTorch models\nmodel2 = timm.create_model('densenet169', pretrained=True)\nmodel4 = timm.create_model('pvt_v2_b5', pretrained=True)\nmodel3 = timm.create_model('vit_base_patch16_224.orig_in21k_ft_in1k', pretrained=True)\nmodel1 = timm.create_model('deit_base_patch16_224.fb_in1k', pretrained=True)\n\n\nimport torch\nimport torch.nn as nn\n\n# Function to modify the model\ndef modify_model(model):\n    # Freeze all parameters in the existing model\n    for param in model.parameters():\n        param.requires_grad = False\n\n    if hasattr(model, 'classifier'):\n        # Get the number of input features of the existing classifier layer\n        n_inputs = model.classifier.in_features\n        # Modify the classifier layer for binary classification\n        model.classifier = nn.Sequential(\n            nn.Linear(n_inputs, 2048),\n            nn.BatchNorm1d(2048),  # Batch Normalization\n            nn.ReLU(),\n            nn.Dropout(0.2),  # Dropout for regularization\n\n            nn.Linear(2048, 1024),  # Increase the number of neurons\n            nn.BatchNorm1d(1024),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n\n            nn.Linear(1024, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n\n            nn.Linear(512, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n\n            nn.Linear(256, 128),\n            nn.BatchNorm1d(128),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n\n            nn.Linear(128, 64),\n            nn.BatchNorm1d(64),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n\n            nn.Linear(64, 6)\n        )\n    elif hasattr(model, 'head'):\n        # Get the number of input features of the existing head layer\n        n_inputs = model.head.in_features\n        # Modify the head layer for binary classification\n        model.head = nn.Sequential(\n            nn.Linear(n_inputs, 2048),\n            nn.BatchNorm1d(2048),  # Batch Normalization\n            nn.ReLU(),\n            nn.Dropout(0.2),  # Dropout for regularization\n\n            nn.Linear(2048, 1024),  # Increase the number of neurons\n            nn.BatchNorm1d(1024),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n\n            nn.Linear(1024, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n\n            nn.Linear(512, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n\n            nn.Linear(256, 128),\n            nn.BatchNorm1d(128),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n\n            nn.Linear(128, 64),\n            nn.BatchNorm1d(64),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n\n            nn.Linear(64, 6)\n        )\n    elif hasattr(model, 'fc'):\n        # Get the number of input features of the existing fc layer\n        n_inputs = model.fc.in_features\n        # Modify the fc layer for binary classification\n        model.fc = nn.Sequential(\n            nn.Linear(n_inputs, 2048),\n            nn.BatchNorm1d(2048),  # Batch Normalization\n            nn.ReLU(),\n            nn.Dropout(0.2),  # Dropout for regularization\n\n            nn.Linear(2048, 1024),  # Increase the number of neurons\n            nn.BatchNorm1d(1024),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n\n            nn.Linear(1024, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n\n            nn.Linear(512, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n\n            nn.Linear(256, 128),\n            nn.BatchNorm1d(128),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n\n            nn.Linear(128, 64),\n            nn.BatchNorm1d(64),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n\n            nn.Linear(64, 6)\n        )\n\n    return model\n\n# Example usage:\nmodel1 = modify_model(model1)\nmodel2 = modify_model(model2)\nmodel3 = modify_model(model3)\nmodel4 = modify_model(model4)\n\n# Move models to device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodels = [model1, model2]#, model3, model4]\nfor model in models:\n    model.to(device)\n\n\n# Load pre-trained model weights\nmodel_paths = [\n    \"/kaggle/input/10x-deigset/models/DeiT/model_weights_epoch_199.pth\",\n    \"/kaggle/input/10x-deigset/models/DenseNet169/model_weights_epoch_199_DenseNet169.pth\",\n    \"/kaggle/input/10x-deigset/models/ViT/model_weights_epoch_199_vit.pth\",\n    \"/kaggle/input/10x-deigset/models/pvt/model_weights_epoch_199_pvt.pth\"\n]\n\nfor model, path in zip(models, model_paths):\n    model.load_state_dict(torch.load(path))\n\n# Set models to evaluation mode\nfor model in models:\n    model.eval()\n# Get data loaders\ntrain_loader, val_loader, test_loader = get_data_loaders(\"/kaggle/input/10x-diaga-1200/10x/\", \"/kaggle/input/diag-10x-500/10X/Test\", batch_size=128)\n# Step 5: Extract features from pre-trained models\ndef extract_features(model, data_loader):\n    model.eval()\n    features = []\n    labels = []\n    with torch.no_grad():\n        for images, targets in data_loader:\n            images = images.to(device)\n            features.append(model(images).cpu().numpy())\n            labels.append(targets.numpy())\n    features = np.concatenate(features)\n    labels = np.concatenate(labels)\n    return features, labels\n\ntrain_features = []\nval_features = []\ntest_features = []\ntrain_labels = []\nval_labels = []\ntest_labels = []\n\nfor model in models:\n    train_feats, train_lbls = extract_features(model, train_loader)\n    val_feats, val_lbls = extract_features(model, val_loader)\n    test_feats, test_lbls = extract_features(model, test_loader)\n    train_features.append(train_feats)\n    val_features.append(val_feats)\n    test_features.append(test_feats)\n    train_labels.append(train_lbls)\n    val_labels.append(val_lbls)\n    test_labels.append(test_lbls)\n\n# Concatenate features\ntrain_features = np.concatenate(train_features, axis=1)\nval_features = np.concatenate(val_features, axis=1)\ntest_features = np.concatenate(test_features, axis=1)\ntrain_labels = train_labels[0]  # Assuming all models have the same labels\nval_labels = val_labels[0]\ntest_labels = test_labels[0]\n\n# Step 6: Train the ELM classifier\nelm_pipeline = make_pipeline(StandardScaler(), GaussianRandomProjection(n_components=1000), MLPClassifier(hidden_layer_sizes=(1000,), activation='relu'))\nelm_pipeline.fit(train_features, train_labels)\n\n# Step 7: Evaluate the ELM classifier\ntrain_preds = elm_pipeline.predict(train_features)\nval_preds = elm_pipeline.predict(val_features)\ntest_preds = elm_pipeline.predict(test_features)\n\ntrain_acc = accuracy_score(train_labels, train_preds)\nval_acc = accuracy_score(val_labels, val_preds)\ntest_acc = accuracy_score(test_labels, test_preds)\n\nprint(\"ELM Classifier Performance:\")\nprint(f\"Train Accuracy: {train_acc:.4f}\")\nprint(f\"Validation Accuracy: {val_acc:.4f}\")\nprint(f\"Test Accuracy: {test_acc:.4f}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]}]}